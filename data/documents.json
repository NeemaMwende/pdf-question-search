{
  "documents": [
    {
      "id": "43563374-f29a-48cc-ad18-786c7a1105bb",
      "filename": "All_Tied_Up_in_You.pdf",
      "text": "\n\nAll Tied Up in You \nPosted originally on the Archive of Our Own at http://archiveofourown.org/works/60224.\nRating:\nExplicit\nArchive Warning:\nNo Archive Warnings Apply\nCategory:\nM/M\nFandom:\nSupernatural\nRelationship:\nDean Winchester/Sam Winchester\nCharacters:\nDean Winchester, Sam Winchester\nAdditional Tags:\nWincest - Freeform, Bondage\nLanguage:\nEnglish\nStats:\nPublished: 2010-02-06 Words: 1,771 Chapters: 1/1\n\nAll Tied Up in You\nby Zilchtastic\nSummary\nSam pulls against the ropes, a fine trembling starting in his arms as he grits his teeth a little,\ntests the strength of his bonds. They don't give very much-- his arms are pulled tight to the\nbed, and his legs are just loose enough that he can bend his knees a little. Not that it does him\nany good. Sam isn't going anywhere, not anytime soon.\nNotes\nCo-written with danny_sama on Livejournal.\nPWP! Be ye warned.\n\nThis isn't the way they normally do it.\nSam pulls against the ropes, a fine trembling starting in his arms as he grits his teeth a little,\ntests the strength of his bonds. They don't give very much-- his arms are pulled tight to the\nbed, and his legs are just loose enough that he can bend his knees a little. Not that it does him\nany good. Sam isn't going anywhere, not anytime soon.\nHe can't seem to stop himself from getting worked up by that thought, breath coming faster\nas he grows frustratedly hard, straining against the material of his jeans, gasping when the\nmovement of his hips makes his jeans shift a little, rubbing his cock. This is dangerous--\nHell, this is stupid and he knows it. He's helpless in this position, vulnerable, things no hunter\ncan afford to be. Anyone could come in and find him. Anyone could take advantage of this.\nThat's why it's both a comfort and a torment to have Dean sitting across the motel room in\none of the narrow desk chairs, watching him.\nDean sits, chin in hand and eyes far too intent. He acts like he can just watch for as long as he\nlikes while Sam writhes, tugs at the ropes, moans miserably when they won't give. He acts\nlike he's got all fucking night to watch this show and he intends to get his money's worth.\nSam groans. \"Dean, you bastard.\" He keeps writhing, mostly because he can't not, not with\nDean watching him like that, and his dick is driving him crazy, crazy good, and he might just\nlose it this way. He can't help his own startled jerk at that thought; wetness smears the inside\nof his jeans like a future promise.\nAnd Dean watches, just this side of smug now. Damn him, he has to know that Sam's on the\nverge of coming without a hand on him, that just knowing Dean is over there getting off on\nthe way he looks is almost enough to send him over the edge all on his own. Except it's not\nenough, not what he wants-- not Dean, and it chafes too, dammit.\nSo maybe he starts begging for it. \"Dean, Dean. C'mon, please.\" Sam can feel his face go hot,\nshame and arousal mixing dizzily in his brain. Dean slides out of his chair to stalk closer, to\nstand over his brother and stare down at him as he writhes, lips pulled into a wide smirk, but\nhis eyes are so intense, hot and dark, the green swallowed up by his pupils blown wide.\nDean reaches out and slides a finger-- just one finger-- down Sam's leg, from knee to thigh,\ntracing the heavy inseam of his jeans.\nSam shudders hard, all of him tuning into that single point of contact, lets his eyes flutter\nclosed. Something so innocuous shouldn't feel so intense, shouldn't make him want so badly\nthat he's willing to beg again, promise anything, anything, just so Dean will touch him more.\nHe has to clear his throat twice just to rasp, \"God, Dean, please.\" Dean smiles, bright and\nwicked.\nAnd then just like that, the contact is gone. Sam keens in his throat, straining upward to find\nit again.\n\nDean sounds sort of awed as he stares down. \"Sam, Sammy. You don't know how good you\nlook like this. Fuck.\"\nSam spreads his legs just that fraction wider that the ropes allow, all desperate invitation.\nDean, Dean. You know you can have it. The waiting is driving him out of his head. He lets his\neyes go heavy and he breathes out, \"Dean, now.\" Not forcefully, not even a command, but\nwhatever it is Dean reads in his face, it's enough to make him slide onto the bed and lean\ndown to catch Sam's mouth in a hard and breathless kiss.\nSam is pliant at first, whimpering slightly, lips parting to let Dean lick into his mouth, fast\nand eager, and then he pushes up against him, wanting to feel as much as he can. His fingers\ntwitch with the need to feel Dean's hair between them, the short prickles and the rasp of the\nshorter stubble at the back of his neck, and something almost like anger heats in the pit of his\nstomach when his hands stay firmly tied and he can't. Dean shifts then, slides his body easily\nbetween Sam's spread legs, lets Sam take more of his weight as he settles there. Dean is hard,\noh god, Sam can feel it, and he suddenly couldn't give a fuck about Dean's hair, because he\ncan't give a fuck about anything, not when his mind is completely desperately blank.\nHe stops fighting then, all the struggle sliding out of him as he just goes loose and willing\nunder his brother. Dean notices the change, slides back just enough to whisper hot and wet\nagainst his mouth, \"Yeah, Sammy, that's it. You want this bad, don't you?\"\nSam can't even force out the words, so he just nods. Dean lowers himself back down to\nmouth at his throat-- Sam throws his head back, helplessly, pleadingly, hissing between his\nteeth when Dean follows up the sharp sting of a bite with the soothing wetness of his tongue.\nDean shifts a little farther down, following the line of Sam's throat, lips and tongue tracing\nover his adam's apple, which moves under his mouth as Sam sucks in another sharp breath.\nThe hollow between his clavicles catches Dean's attention next, followed by a long, wet, dirty\nlick along Sam's collarbones as if Dean wants to distract him from the hand that's working its\nway up under his shirt.\nSam can't decide if he's going nuts in a good way or a bad way, because he's so far past\nwanting foreplay, he just needs Dean, now, now. Then Dean's fingers beneath his shirt find\nhis nipple and twist, just so, and Sam can't stop the high, desperate sound that he makes.\nDean breathes out a hot gust against his skin. \"Fuck, Sam. You make those noises for me, and\nI--\" He doesn't finish the thought, just twists again, and then drags blunt nails slow and\ninexorable down Sam's ribs, making him squirm and thrust up mindlessly. He can't stop\npulling at the ropes-- Sam can feel them digging into his skin, but the rub and the ache is so\ndistant, so unimportant now.\n\"Dean,\" Sam breathes, panting so quick he almost sounds afraid. \"Dean, c'mon, please, just\nplease--\"\n\"Please what, Sam?\" As if he doesn't know.\nSam just whimpers, turns his face away, and Dean stops everything, leans up again,\nwhispering hot and urgent in his ear, \"Say it, Sam. More than anything right now I wanna\n\nhear you say it.\"\nSam trembles under him like he's going to fall apart, like he'll just shake himself to pieces\nthat Dean will gently put back together later. \"Dean,\" he whispers finally, voice nearly ruined,\n\"fuck me.\"\nDean smiles then, and the very small unclouded part of Sam's very preoccupied brain\nwonders how a Dean-smile can be so dirty and so tender at the same time. \"No.\"\n\"What?\" Sam's brain refuses to shift gears and splutters out by the roadside. He jerks under\nDean, mindlessly. \"Dean, you have to, god, don't do this to me--\"\n\"Do what to you? Leave you like this? I totally could.\" His fingers trace down to Sam's wrist,\nskirting along the edges of the ropes feather-light. \"You're not goin' anywhere unless I let\nyou, Sammy.\"\n\"I know, I know.\" Sam feels dizzy, hot. More precome wets the inside of his jeans, and he's\ndesperate not to lose it like this. \"Anything, Dean, anything you want--\"\nDean shifts, crawling backward down his lap, fingers skimming their way down Sam's chest\nand stomach and then shoving his shirt up roughly. \"When you ask me so pretty,\" Dean\nmurmurs, almost like he's talking to himself, \"how can I say no?\" His hands start working on\nthe buttons of Sam's jeans, and Sam can only arch his back in frantic encouragement. Having\nDean's hands right there is almost too much already, and when Dean tugs the button open\nonly to stop there, Sam lets out a cry of frustration.\n\"Dean, fuck, Dean--\"\n\"Gonna come like this?\" Dean bends himself low, till his mouth is hovering over the tight\nbulge in Sam's jeans. \"Gonna spill right in your pants for me, Sammy?\" His lips tilt sideways.\n\"Maybe if you ask real nice, I'll lick you clean after.\"\nSam closes his eyes, breathes fast and desperate as Dean lowers that last inch and mouths at\nthe front of his pants. Warmth seeps through the material, warmth and wetness and then\npressure as Dean sucks at him hard right through the cotton.\n\"Ohgod ohgod ohgod--\" It's too much. The unending tease takes its toll, finally, and Sam can\nonly ride it as he comes, gasping and writhing and spitting Dean's name. His cock jerks,\nspilling hotly, come soaking into his unbuttoned jeans, and it's the worst and also the best\nthing he's felt in a long, long time.\nReality filters back slowly. Sam shakes his head, trying to dislodge some of the hair sticking\ndamply to his forehead. Up above the ceiling comes gradually into focus, and it's an attempt\nto get his breathing back under control that has Sam counting the cracks in the plaster.\n\"Can't believe you really creamed your pants,\" Dean says, sounding way too fucking pleased.\nSam lifts his head to glare at his brother, but the expression just takes too much effort,\nespecially when Dean starts tracing his fingers over the wet patch of his jeans. He lets his\nhead flop back to the pillows, groaning miserably.\n\n\"Jerk. All your fault,\" he rasps.\nDean hums out a pleased sound and starts on Sam's zipper. \"Guess I can take responsibility\nfor this one.\"\nSam shudders hard-- the contact is too much, too soon. \"Nnh, Dean, what're you--\"\n\"Promised to lick you clean, didn't I? I'm gonna.\"\nWay too much, way too soon. Something too sharp to be arousal but too good to be anything\nelse sparks its way up Sam's spine. \"Dean, wait, what about you?\"\nDean laughs, warm and self-satisfied like when he knows what he's going to get and it's a\nsure thing. \"I'm gonna lick you hard again, Sammy,\" he says, voice sliding low and dark,\n\"and when you're ready for me again I'm gonna ride you till we both come.\"\nSam thinks it's the best idea either of them has had all night.\n\nPlease drop by the Archive and comment to let the creator know if you enjoyed their work!",
      "questions": [
        "Dean, you bastard.",
        "Dean, c'mon, please.",
        "Yeah, Sammy, that's it. You want this bad, don't you?",
        "Dean, you make those noises for me, and I--",
        "Dean, c'mon, please, just please--",
        "Dean, you have to, god, don't do this to me--",
        "Gonna spill right in your pants for me, Sammy?",
        "Can't believe you really creamed your pants?",
        "Jerk. All your fault?"
      ],
      "answers": {
        "Can't believe you really creamed your pants?": "Yes, the phrase \"Can't believe you really creamed your pants\" is mentioned in the text."
      }
    },
    {
      "id": "c3850ec8-94ff-4f2d-b9e6-1eff7cdbb290",
      "filename": "Artificial Intelligence-Machine Learning Explained.pdf",
      "text": "\n\n \n   \nArtificial Intelligence/ \nMachine Learning Explained \nGordian Knot Center for \nNational Security Innovation \nAuthor: Steve Blank \nhttps://gordianknot.stanford.edu \n\n \nArtificial Intelligence/Machine Learning– Explained \n \nAI is a once-in-a lifetime commercial and defense game changer \n \nHundreds of billions in public and private capital is being invested in AI and Machine Learning \ncompanies. The number of patents filed in 2021 is more than 30 times higher than in 2015 as \ncompanies and countries across the world have realized that AI and Machine Learning will be a \nmajor disruptor and potentially change the balance of military power.  \n \nUntil recently, the hype exceeded reality. Today, however, advances in AI in several important \nareas (here, here, here, here and here) equal and even surpass human capabilities.   \n \nIf you haven’t paid attention, now’s the time.  \n \nAI and the DoD \nThe Department of Defense has thought that AI is such a foundational set of technologies that \nthey started a dedicated organization- the JAIC - to enable and implement artificial intelligence \nacross the Department. They provide the infrastructure, tools, and technical expertise for DoD \nusers to successfully build and deploy their AI-accelerated projects. \n \nSome specific defense related AI applications are listed later in this document. \n \nWe’re in the Middle of a Revolution \nImagine it’s 1950, and you’re a visitor who traveled back in time from today. Your job is to \nexplain the impact computers will have on business, defense and society to people who are \nusing manual calculators and slide rules. You succeed in convincing one company and a \ngovernment to adopt computers and learn to code much faster than their competitors \n/adversaries. And they figure out how they could digitally enable their business – supply chain, \ncustomer interactions, etc. Think about the competitive edge they’d have by today in business \nor as a nation. They’d steamroll everyone. \n \nThat’s where we are today with Artificial Intelligence and Machine Learning. These technologies \nwill transform businesses and government agencies. Today, 100s of billions of dollars in private \ncapital have been invested in 1,000s of AI startups. The U.S. Department of Defense has created \na dedicated organization to ensure its deployment. \n \nBut What Is It? \nCompared to the classic computing we’ve had for the last 75 years, AI has led to new types of \napplications, e.g. facial recognition; new types of algorithms, e.g. machine learning; new types \nof computer architectures,  e.g. neural nets; new hardware, e.g. GPUs; new types of software \ndevelopers, e.g. data scientists; all under the overarching theme of artificial intelligence. The \nsum of these feels like buzzword bingo. But they herald a sea change in what computers are \ncapable of doing, how they do it, and what hardware and software is needed to do it. \n\n \n \nThis brief will attempt to describe all of it. \n \nNew Words to Define Old Things \nOne of the reasons the world of AI/ML is confusing is that it’s created its own language and \nvocabulary. It uses new words to define programming steps, job descriptions, development \ntools, etc. But once you understand how the new world maps onto the classic computing world, \nit starts to make sense. So first a short list of some key definitions. \n \nAI/ML - a shorthand for Artificial Intelligence/Machine Learning \n \nArtificial Intelligence (AI) - a catchall term used to describe “Intelligent machines” which can \nsolve problems, make/suggest decisions and perform tasks that have traditionally required \nhumans to do. AI is not a single thing, but a constellation of different technologies.  \n \nMachine Learning (ML) - a subfield of \nartificial intelligence. Humans \ncombine data with algorithms (see \nhere for a list) to train a model using \nthat data. This trained model can \nthen make predications on new data \n(is this picture a cat, a dog or a \nperson?) or decision-making \nprocesses (like understanding text \nand images) without being explicitly \nprogrammed to do so.  \n \nMachine learning algorithms - computer programs that adjust themselves to perform \nbetter as they are exposed to more data. The “learning” part of machine learning means \nthese programs change how they process data over time. In other words, a machine-\nlearning algorithm can adjust its own settings, given feedback on its previous \nperformance in making predictions about a collection of data (images, text, etc.).  \n \nDeep Learning/Neural Nets – a subfield of machine learning. Neural networks make up the \nbackbone of deep learning. (The “deep” in deep learning \nrefers to the depth of layers in a neural network.) Neural \nnets are effective at a variety of tasks (e.g., image \nclassification, speech recognition). A deep learning neural \nnet algorithm is given massive volumes of data, and a task to \nperform - such as classification. The resulting model is \ncapable of solving complex tasks such as recognizing objects \nwithin an image and translating speech in real time. In \nreality, the neural net is a logical concept that gets mapped \nonto a physical set of specialized processors. See here.) \n\n \nData Science – a new field of computer science. Broadly it encompasses data systems and \nprocesses aimed at maintaining data sets and deriving meaning out of them. In the context of \nAI, it’s the practice of people who are doing machine learning. \n \nData Scientists - responsible for extracting insights that help businesses make decisions. \nThey explore and analyze data using machine learning platforms to create models about \ncustomers, processes, risks, or whatever they’re trying to predict. \n \nWhat’s Different? Why is Machine Learning Possible Now? \nTo understand why AI/Machine Learning can do these things, let’s compare them to computers \nbefore AI came on the scene. (Warning – simplified examples below.) \n \nClassic Computers \nFor the last 75 years computers (we’ll call these classic computers) have both shrunk to pocket \nsize (iPhones) and grown to the size of warehouses (cloud data centers), yet they all continued \nto operate essentially the same way. \n \nClassic Computers - Programming \nClassic computers are designed to do anything a human explicitly tells them to do. People \n(programmers) write software code (programming) to develop applications, thinking a priori \nabout all the rules, logic and knowledge that need to be built in to an application so that it can \ndeliver a specific result. These rules are explicitly coded into a program using a software \nlanguage (Python, JavaScript, C#, Rust, ...).  \n \nClassic Computers -  Compiling \nThe code is then compiled using software to translate the programmer’s source code into a \nversion that can be run on a target computer/browser/phone. For most of today’s programs, \nthe computer used to develop and compile the code does not have to be that much faster than \nthe one that will run it. \n\n \n \nClassic Computers - Running/Executing Programs \nOnce a program is coded and compiled, it can be deployed and run (executed) on a desktop \ncomputer, phone, in a browser window, a data center cluster, in special hardware, etc. \nPrograms/applications can be games, social media, office applications, missile guidance \nsystems, bitcoin mining, or even operating systems e.g. Linux, Windows, IOS. These programs \nrun on the same type of classic computer architectures they were programmed in.  \n \nClassic Computers – Software Updates, New Features  \nFor programs written for classic computers, software developers receive bug reports, monitor \nfor security breaches, and send out regular software updates that fix bugs, increase \nperformance and at times add new features.  \n \nClassic Computers-  Hardware \nThe CPUs (Central Processing Units) that write and run these Classic Computer applications all \nhave the same basic design (architecture). The CPUs are designed to handle a wide range \nof tasks quickly in a serial fashion. These CPUs range from Intel X86 chips, and the ARM cores \non Apple M1 SoC, to the z15 in IBM mainframes. \n \nMachine Learning \nIn contrast to programming on classic computing with fixed rules, machine learning is just like it \nsounds – we can train/teach a computer to “learn by example” by feeding it lots and lots of \nexamples. (For images a rule of thumb is that a machine learning algorithm needs at least 5,000 \nlabeled examples of each category in order to produce an AI model with decent performance.) \nOnce it is trained, the computer runs on its own and can make predictions and/or complex \ndecisions.   \n \nJust as traditional programming has three steps - first coding a program, next compiling it and \nthen running it  - machine learning also has three steps: training (teaching), pruning and \ninference (predicting by itself.)  \n \n\n \nMachine Learning - Training \nUnlike programing classic computers with explicit rules, training is the process of “teaching” a \ncomputer to perform a task e.g. recognize faces, signals, understand text, etc. (Now you know \nwhy you're asked to click on images of traffic lights, cross walks, stop signs, and buses or type \nthe text of scanned image in ReCaptcha.) Humans provide massive volumes of “training data” \n(the more data, the better the model’s performance) and select the appropriate algorithm to \nfind the best optimized outcome.  \n(See the detailed “machine learning pipeline” later in this section for the gory details.) \n \n  \nBy running an algorithm selected by a data scientist on a set of training data, the Machine \nLearning system generates the rules embedded in a trained model. The system learns from \nexamples (training data), rather than being explicitly programmed. (See the “Types of Machine \nLearning” section for more detail.) This self-correction is pretty cool. An input to a neural net \nresults in a guess about what that input is. The neural net then takes its guess and compares it \nto a ground-truth about the data, effectively asking an expert “Did I get this right?” The \ndifference between the network’s guess and the ground truth is its error. The network \nmeasures that error, and walks the error back over its model, adjusting weights to the extent \nthat they contributed to the error.) \n \nJust to make the point again: The algorithms combined with the training data - not external \nhuman computer programmers - create the rules that the AI uses. The resulting model is \ncapable of solving complex tasks such as recognizing objects it’s never seen before, translating \ntext or speech, or controlling a drone swarm.   \n \n\n \n(Instead of building a model from scratch you can now buy, for common machine learning \ntasks, pretrained models from others and here, much like chip designers buying IP Cores.) \n \nMachine Learning Training - Hardware  \nTraining a machine learning model is a very computationally intensive task. AI hardware must \nbe able to perform thousands of multiplications and additions in a mathematical process called \nmatrix multiplication. It requires specialized chips to run fast. (See the AI hardware section for \ndetails.)  \n \nMachine Learning - Simplification via pruning, quantization, distillation \nJust like classic computer code needs to be compiled and optimized before it is deployed on its \ntarget hardware, the machine learning models are simplified and modified (pruned) to use less \ncomputing power, energy, and  memory before they’re deployed to run on their hardware. \n \nMachine Learning – Inference Phase \nOnce the system has been trained it can be copied to other devices and run. And the computing \nhardware can now make inferences (predictions) on new data that the model has never \nseen before.  \n \nInference can even occur locally on edge devices where physical devices meet the digital world \n(routers, sensors, IOT devices), close to the source of where the data is generated. This reduces \nnetwork bandwidth issues and eliminates latency issues.  \n \nMachine Learning Inference - Hardware  \nInference (running the model) requires substantially less compute power than training. But \ninference also benefits from specialized AI chips. \n \nMachine Learning – Performance Monitoring and Retraining  \nJust like classic computers where software developers do regular software updates to fix bugs \nand increase performance and add features, machine learning models also need to be updated \nregularly by adding new data to the old training pipelines and running them again. Why?  \n\n \n \nOver time machine learning models get stale. Their real-world performance generally degrades \nover time if they are not updated regularly with new training data that matches the changing \nstate of the world. The models need to be monitored and retrained regularly for data and/or \nconcept drift, harmful predictions, performance drops, etc. To stay up to date, the models need \nto re-learn the patterns by looking at the most recent data that better reflects reality.  \n \nOne Last Thing – “Verifiability/Explainability” \nUnderstanding how an AI works is essential to fostering trust and confidence in AI production \nmodels. \n \nNeural Networks and Deep Learning differ from other types of Machine Learning algorithms in \nthat they have low explainability. They can generate a prediction, but it is very difficult to \nunderstand or explain how it arrived at its prediction. This “explainability problem” is often \ndescribed as a problem for all of AI, but it’s primarily a problem for Neural Networks and Deep \nLearning. Other types of Machine Learning algorithms – for example decision trees – have very \nhigh explainability. The results of the five-year DARPA Explainable AI Program (XAI) are worth \nreading here. \n \n \n \nSo   What Can Machine Learning Do?\n1\n \nIt’s taken decades but as of today, on its simplest implementations, machine learning \napplications can do some tasks better and/or faster than humans.\n Machine Learning is most \nadvanced and widely applied today in processing text (through Natural Language Processing) \n \n1\n https://databricks.com/discover/pages/the-democratization-of-artificial-intelligence-and-  deep  -learning \n\n \nfollowed by understanding images and videos (through Computer Vision) and analytics and \nanomaly detection. For example: \n \nRecognize and Understand Text/Natural Language Processing \nAI is better than humans on basic reading comprehension benchmarks like SuperGLUE and \nSQuAD and their performance on complex linguistic tasks is almost there. Applications: GPT-3, \nM6, OPT-175B, Google Translate, Gmail Autocomplete, Chatbots, Text summarization.  \n \nWrite Human-like Answers to Questions and Assist in Writing Computer Code \nAn AI can write original text that is indistinguishable from that created by humans. Examples \nGPT-3, Wu Dao 2.0 or generate computer code. Example GitHub Copilot, Wordtune \n \nRecognize and Understand Images and video streams \nAn AI can see and understand what it sees. It can identify and detect an object or a \nfeature in an image or video. It can even identify faces. It can scan news broadcasts \nor read and assess text that appears in videos. It has uses in threat detection -  \nairport security, banks, and sporting events. In medicine to interpret MRI’s or to \ndesign drugs. And in retail to scan and analyze in-store imagery to intuitively determine \ninventory movement. Examples of ImageNet benchmarks here and here \n \nDetect Changes in Patterns/Recognize Anomalies \nAn AI can recognize patterns which don’t match the behaviors expected for a \nparticular system, out of millions of different inputs or transactions. These \napplications can discover evidence of an attack on financial networks, fraud \ndetection in insurance filings or credit card purchases; identify fake reviews; even tag \nsensor data in industrial facilities that mean there’s a safety issue. Examples here, here and \nhere.  \n \n Power Recommendation Engines \n An AI can provide recommendations based on user behaviors used in ecommerce \nto provide accurate suggestions of products to users for future purchases based on \ntheir shopping history. Examples: Alexa and Siri\n \n \nRecognize and Understand Your Voice \nAn AI can understand spoken language. Then it can comprehend what is being said and in what \ncontext. This can enable chatbots to have a conversation with \npeople. It can record and transcribe meetings. (Some versions can \neven read lips to increase accuracy.) Examples: Siri/Alexa/Google \nAssistant. Example here \n \nCreate Artificial Images \nAI can create artificial images (DeepFakes) that are \nindistinguishable from real ones using Generative Adversarial \n\n \nNetworks. Useful in entertainment, virtual worlds, gaming, fashion design, etc. Synthetic faces \nare now indistinguishable and more trustworthy than photos of real people. Paper here. \n \nCreate Artist Quality Illustrations from A Written Description  \nAI can generate images from text descriptions, creating anthropomorphized versions of animals \nand objects, combining unrelated concepts in plausible ways. An example is Dall-E \n \nGenerative Design of Physical Products  \nEngineers can input design goals into AI-driven generative design software, along with \nparameters such as performance or spatial requirements, materials, manufacturing methods, \nand cost constraints. The software explores all the possible permutations of a solution, quickly \ngenerating design alternatives Example here.   \n \nSentiment Analysis \nAn AI leverages deep natural language processing, text analysis, and computational linguistics \nto gain insight into customer opinion, understanding of consumer sentiment, and \nmeasuring the impact of marketing strategies. Examples: Brand24, MonkeyLearn \n \n \n \nWhat Does this Mean for Businesses? \nSkip this section if you’re interested in national security applications \n \nHang on to your seat. We’re just at the beginning of the revolution. The next phase of AI, \npowered by ever increasing powerful AI hardware and cloud clusters, will combine some of \nthese basic algorithms into applications that do things no human can. It will transform business \nand defense in ways that will create new applications and opportunities.  \n \nHuman-Machine Teaming \nApplications with embedded intelligence have already begun to appear thanks to massive \nlanguage models. For example - Copilot as a pair-programmer in Microsoft Visual Studio \nVSCode. It’s not hard to imagine DALL-E 2 as an illustration assistant in a photo editing \napplication, or GPT-3 as a writing assistant in Google Docs. \n  \nAI in Medicine \nAI applications are already appearing in radiology, dermatology, and oncology. Examples: IDx-\nDR, OsteoDetect, Embrace2.  AI Medical image identification can automatically detect lesions, \nand tumors with diagnostics equal to or greater than humans. For Pharma, AI will power drug \ndiscovery design for finding new drug candidates. The FDA has a plan for approving AI software \nhere has a list of AI-enabled medical devices here. \n \n\n \nAutonomous Vehicles \nHarder than it first seemed, but car companies like Tesla will eventually get better than human \nautonomy for highway driving and eventually city streets. \n \nDecision support \nAdvanced virtual assistants can listen to and observe behaviors, build and maintain data \nmodels, and predict and recommend actions to assist people with and automate tasks that \nwere previously only possible for humans to accomplish. \n \nSupply chain management \nAI applications are already appearing in predictive maintenance, risk management, \nprocurement, order fulfillment, supply chain planning and promotion management. \n \nMarketing  \nAI applications are already appearing in real-time personalization, content and media \noptimization and campaign orchestration to augment, streamline and automate marketing \nprocesses and tasks constrained by human costs and capability, and to uncover new customer \ninsights and accelerate deployment at scale. \n \nMaking business smarter: Customer Support \nAI applications are already appearing in virtual customer assistants with speech recognition, \nsentiment analysis, automated/augmented quality assurance and other technologies providing \ncustomers with 24/7 self- and assisted-service options across channels. \n \nAI in  National Security\n2\n \nMuch like the dual-use/dual-nature of classical computers AI developed for commercial \napplications can also be used for national security.  \n \nAI/ML and Ubiquitous Technical Surveillance \nAI/ML have made most cities untenable for traditional \ntradecraft. Machine learning can integrate travel data \n(customs, airline, train, car rental, hotel, license plate \nreaders...,) integrate feeds from CCTV cameras for facial \nrecognition and gait recognition, breadcrumbs from wireless \ndevices and then combine it with DNA sampling. The result \nis automated persistent surveillance. \n \nChina’s employment of AI as a tool of repression and \nsurveillance of the Uyghurs is a dystopian of how a totalitarian regimes will use AI-enable \nubiquitous surveillance to repress and monitor its own populace. \n \n \n2\n https://www.nscai.gov/2021-final-report/ \n\n \nAI/ML on the Battlefield \nAI will enable new levels of performance and autonomy for weapon systems. Autonomously \ncollaborating assets (e.g., drone swarms, ground vehicles) that can coordinate attacks, ISR \nmissions, & more.  \n \nFusing and making sense of sensor data (detecting threats in optical /SAR imagery, classifying \naircraft based on radar returns, searching for anomalies in radio frequency signatures, etc.) \nMachine learning is better and faster than humans in finding targets hidden in a high-clutter \nbackground. Automated target detection and fires from satellite/UAV.  \n \nFor example, an Unmanned Aerial Vehicle (UAV) or Unmanned Ground Vehicles with on board \nAI edge computers could use deep learning to detect and locate concealed chemical, biological \nand explosives threats by fusing imaging sensors and chemical/biological sensors.  \nOther examples include:  \n \nUse AI/ML countermeasures against adversarial, low probability of intercept/low probability of \ndetection (LPI/LPD) radar techniques in radar and communication systems. \n \nGiven sequences of observations of unknown radar waveforms from arbitrary emitters without \na priori knowledge, use machine learning to develop behavioral models to enable inference of \nradar intent and threat level, and to enable prediction of future behaviors.  \n \nFor objects in space, use machine learning to predict and characterize a spacecrafts possible \nactions, its subsequent trajectory, and what threats it can pose from along that trajectory. \nPredict the outcomes of finite burn, continuous thrust, and impulsive maneuvers. \n \nAI empowers other applications such as: \n• Flight Operations Planning Decision Aid Tool for Strike Operations Aboard Aircraft \nCarriers \n• Automated Battle management – air and missile defense, army/navy tactical... \n \nAI/ML in Collection  \nThe front end of intelligence collection platforms has created a firehose of data that have \noverwhelmed human analysts. “Smart” sensors coupled with inference engines that can pre-\nprocess raw intelligence and prioritize what data to transmit and store –helpful in degraded or \nlow-bandwidth environments. \n \nHuman-Machine Teaming in Signals Intelligence \nApplications with embedded intelligence have already begun to appear in commercial \napplications thanks to massive language models. For example - Copilot as a pair-programmer in \n\n \nMicrosoft Visual Studio VSCode. It’s not hard to imagine an AI \nthat can detect and isolate anomalies and other patterns of \ninterest in all sorts of signal data faster and more reliably \nthan human operators.  \n \nAI-enabled natural language processing, computer vision, \nand audiovisual analysis can vastly reduce manual data \nprocessing. Advances in speech-to-text transcription and \nlanguage analytics now enable reading comprehension, \nquestion answering, and automated summarization of large quantities of text. This not only \nprioritizes the work of human analysts, it’s a major force multiplier \n \nAI can also be used to automate data conversion such as translations and decryptions, \naccelerating the ability to derive actionable insights. \n \nHuman-Machine Teaming in Tasking and Dissemination \nAI-enabled systems will automate and optimize tasking and collection for platforms, sensors, \nand assets in near-real time in response to dynamic intelligence requirements or changes in the \nenvironment.  \n \nAI will be able to automatically generate machine-readable versions of intelligence products \nand disseminate them at machine speed so that computer systems across the IC and the \nmilitary can ingest and use them in real time without manual intervention. \n \nHuman-Machine Teaming in Exploitation and Analytics \nAI-enabled tools can augment filtering, flagging, and triage across multiple data sets. They can \nidentify connections and correlations more efficiently and at a greater scale than human \nanalysts, and can flag those findings and the most important content for human analysis. \nAI can fuse data from multiple sources, types of intelligence, and classification levels to produce \naccurate predictive analysis in a way that is not currently possible. This can improve indications \nand warnings for military operations and active cyber defense. \n \nAI/ML Information warfare \nNation states have used AI systems to enhance disinformation campaigns and cyberattacks. \nThis included using “DeepFakes” (fake videos generated by a neural network that are nearly \nindistinguishable from reality). They are harvesting data on Americans to build profiles of our \nbeliefs, behavior, and biological makeup for tailored attempts to manipulate or coerce \nindividuals.  \n \nBut because a large percentage of it is open-source AI is not limited to nation states, AI-\npowered cyber-attacks, deepfakes and AI software paired with commercially available drones \ncan create “poor-man’s smart weapons” for use by rogue states, terrorists and criminals. \n\n \n \nAI/ML Cyberwarfare  \nAI-enabled malware can learn and adapt to a system’s defensive measures, or, conversely, AI-\nenabled cyber-defensive tools can proactively locate and address network anomalies and \nsystem vulnerabilities. \n \n \n \nAI-driven malware, where a malicious logic embeds machine learning methods and models to \nautomatically: (i) probe the target system for inferring \nactionable intelligence (e.g. system configuration or \noperational patterns) and (ii) customize the attack \npayload accordingly (e.g. determine the most opportune \ntime to execute the payload so to maximize the impact).  \n \nAttacks Against AI - Adversarial AI \nAs AI proliferates, defeating adversaries will be predicated on defeating their AI and vice versa. \nAs Neural Networks take over sensor processing and triage tasks, a human may only be alerted \nif the AI deems it suspicious. Therefore, we only need to defeat the AI to evade detection, not \nnecessarily a human.  \n \nAdversarial attacks against AI fall into three types: \n• Data misclassification- to generate false positive or negative results \n• Synthetic data generation-to feed false information  \n• Data analysis – for AI-assisted classical attack generation \n \nAI Attack Surfaces \nElectronic Attack (EA), Electronic Protection (EP), Electronic Support (ES) all have analogues in \nthe AI algorithmic domain. In the future, we may play the same game about the “Algorithmic \nSpectrum,” denying our adversaries their AI capabilities while defending ours. Other can steal \nor poison our models  or manipulate our training data. \nAI –Friend or Foe?\nCHAPTER 1\n45\np\n5IF\u00036\u000f4\u000f\u0003HPWFSONFOU\u0003JT\u0003OPU\u0003QSFQBSFE\u0003UP\u0003EFGFOE\u0003UIF\u00036OJUFE\u00034UBUFT\u0003JO\u0003UIF\u0003\nDPNJOH\u0003BSUJmDJBM\u0003JOUFMMJHFODF\u0003\t\"*\n\u0003FSB\u000f\u0003\"*\u0003BQQMJDBUJPOT\u0003BSF\u0003USBOTGPSNJOH\u0003\nexisting threats, creating new classes of threats, and further emboldening \nstate  and  non-state  adversaries  to  exploit  vulnerabilities  in  our  open \nsociety.\n1\n AI systems will extend the range and reach of adversaries into \nUIF\u00036OJUFE\u00034UBUFT\u0003KVTU\u0003BT\u0003UIF\u0003NJTTJMF\u0003BHF\u0003BOE\u0003UFSSPSJTN\u0003CSPVHIU\u0003UISFBUT\u0003\ncloser  to  home.  Because  of  AI,  adversaries  will  be  able  to  act  with \nmicro-precision, but at macro-scale and with greater speed. They will \nVTF\u0003\"*\u0003UP\u0003FOIBODF\u0003DZCFS\u0003BUUBDLT\u0003BOE\u0003EJHJUBM\u0003EJTJOGPSNBUJPO\u0003DBNQBJHOT\u0003\nand to target individuals in new ways. AI will also help create precisely \nengineered biological agents. And adversaries will manipulate the AI \nsystems we will rely upon.\n  \n\"*\u0003UFDIOPMPHJFT\u0003FYBDFSCBUF\u0003UXP\u0003FYJTUJOH\u0003OBUJPOBM\u0003TFDVSJUZ\u0003DIBMMFOHFT\u001b\u0003\n• First, digital dependence in all walks of life increases vulnerabilities to cyber intrusion \nacross every segment of our society: corporations, universities, government, private \norganizations, and the homes of individual citizens. In parallel, new sensors have \nflooded the modern world. The internet of things (IoT), cars, phones, homes, and social \nmedia platforms collect streams of data, which can then be fed into AI systems that can \nidentify, target, and manipulate or coerce our citizens.\n2\n• Second, state and non-state adversaries are challenging the United States below \nthe threshold of direct military confrontation by using cyber attacks, espionage, \npsychological and political warfare, and financial instruments. Adversaries do not \nneed AI to conduct widespread cyber attacks, exfiltrate troves of sensitive data about \nAmerican citizens, interfere in our elections, or bombard us with malign information on \nGraphic 1.1: How A  I i s T ransf   or ming th  e T  hrea t L an dscape\nCurrent Threats \nAdvanced BY AI Systems\nAI transforms existing \nrange and reach of threats\nSelf-replicating \nAI-generated \nmalware\nImproved \nand autonomous \ndisinformation \ncampaigns\nAI-engineered and \ntargeted pathogens\nNew Threats \nFROM AI Systems\nAI creates new \nthreat phenomena\nThreats TO AI Stacks \nThemselves\nAI itself is also a new \nattack surface\nFuture Threats \nVIA\n AI Systems\nExamples of potential \nthreats to keep in view\nDeepfakes and \ncomputational \npropaganda\nMicro-targeting: \nAI-fused data for \ntargeting or blackmail\nAI swarms and \nnano-swarms\nAI attack involves the \nwhole “AI stack”.  \nExamples include:\n   Model inversion\n   Training data \n   manipulation\n   “Data lake” \n   poisoning\nRapid machine-to-\nmachine escalation \nvia automated C2\nAI-enabled human \naugmentation by \npeer competitors\nProliferation of \nsimple lethal \nautonomous weapons \nto terrorists\n)PX\u0003\"*\u0003JT\u0003\nTra n s fo r m i n g t h e \nThreat Landscape\nsblank@Stanford.edu\nSource:\tFinal\tReport:\tNational\tSecurity\tCommission\ton\tArtificial\tIntelligence\t \n \n\n \n \nWhat Makes AI Possible Now?\n3\n \n \nFour changes make Machine Learning possible now: \n1. Massive Data Sets \n2. Improved Machine Learning algorithms  \n3. Open-Source Code, Pretrained Models and \nFrameworks \n4. More computing power \n \nMassive Data Sets \nMachine Learning algorithms tend to require large quantities of training data in order to \nproduce high-performance AI models. (Training Google’s GPT-3 Natural Language Model with \n175 billion parameters takes 1,024 Nvidia A100 GPUs more than one month.) Today, strategic \nand tactical sensors pour in a firehose of images, signals and other data. Billions of computers, \ndigital devices and sensors connected to the Internet, producing and storing large volumes of \ndata, which provide other sources of intelligence. For example facial recognition requires \nmillions of labeled images of faces for training data. \n \nOf course more data only helps if the data is relevant to your desired application. Training data \nneeds to match the real-world operational data very, very closely to train a high-performing AI \nmodel. \n \nImproved Machine Learning algorithms  \nThe first Machine Learning algorithms are decades old, and some remain incredibly useful. \nHowever, researchers have discovered new algorithms that have greatly sped up the fields \ncutting-edge. These new algorithms have made Machine Learning models more flexible, more \nrobust, and more capable of solving different types of problems. \n \nOpen-Source Code, Pretrained Models and Frameworks \nDeveloping Machine Learning systems required a lot of expertise and custom software \ndevelopment that made it out of reach for most organizations. Now open-source code libraries \nand developer tools allow organizations to use and build upon the work of external \ncommunities. No team or organization has to start from scratch, and many parts that used to \nrequire highly specialized expertise have been automated. Even non-experts and beginners can \ncreate useful AI tools. In some cases, open-source ML models can be entirely reused and \npurchased. Combined with standard competitions, open source, pretrained models and \nframeworks have moved the field forward faster than any federal lab or contractor. It’s been a \nfeeding frenzy with the best and brightest researchers trying to one-up each other to prove \nwhich ideas are best. \n \n \n3\n https://www.ai.mil/docs/Understanding%20AI%20Technology.pdf \n\n \nThe downside is that, unlike past DoD technology development - where the DoD leads it, can \ncontrol it, and has the most advanced technology (like stealth and electronic warfare), in most \ncases the DoD will not have the most advanced algorithms or models. The analogy for AI is \ncloser to microelectronics than it is EW. The path forward for the DoD should be supporting \nopen research, but optimizing on data set collection, harvesting research results, and fast \napplication.  \n \nMore computing power – special chips \nMachine Learning systems require a lot of computing power. Today, it’s possible to run \nMachine Learning algorithms on massive datasets using commodity Graphics Processing Units \n(GPUs). (See the machine learning hardware section below). While many of the AI performance \nimprovements have been due to human cleverness on better models and algorithms, most of \nthe performance gains have been the massive increase in compute performance.  (See the \nsemiconductor section.)   \n \nMore computing power – AI In the Cloud \nThe rapid growth in the size of machine learning models has been achieved by the move to \nlarge data center clusters. The size of machine learning models are limited by time to train \nthem. For example, in training images, the size of the model scales with the number of pixels in \nan image. ImageNet Model sizes are 224x224 pixels. But HD (1920x1080) images require 40x \nmore computation/memory. Large Natural Language Processing models - e.g. summarizing \narticles, English-to-Chinese translation like Google’s GPT-3 require enormous models. GPT-3 \nuses 175 billion parameters and was trained on a cluster with 1,024 Nvidia A100 GPUs that cost \n~$25 million! (Which is why large clusters exist in the cloud, or the largest companies/ \ngovernment agencies.) Facebooks Deep Learning and Recommendation Model (DLRM) was \ntrained on 1TB data and has 24 billion parameters. Some cloud vendors train on >10TB data \nsets. \n \nInstead of investing in massive amounts of computers needed for training companies can use \nthe enormous on-demand, off-premises hardware in the cloud (e.g. Amazon AWS, Microsoft \nAzure) for both training machine learning models and deploying inferences.  \n \nWe’re Just Getting Started \nThe next 10 years will see a massive improvement on AI inference and training capabilities. This \nwill require regular refreshes of the hardware– on the chip and cloud clusters - to take \nadvantage. This is the AI version of Moore’s Law on steroids – applications that are completely \ninfeasible today will be easy in 5 years.  \n \nWhat Can’t AI Do?  \nWhile AI can do a lot of things better than humans when focused on a narrow objective, there \nare many things it still can’t do. AI works well in specific domain where you have lots of data, \ntime/resources to train, domain expertise to set the right goals/rewards during training, but \nthat is not always the case.  \n\n \n \nFor example AI models are only as good as the fidelity and quality of the training data. Having \nbad labels can wreak havoc on your training results. Protecting the integrity of the training data \nis critical.  \n \nIn addition, AI is easily fooled by out-of-domain data (things it hasn’t seen before). This can \nhappen by “overfitting” - when a model trains for too long on sample data or when the model is \ntoo complex, it can start to learn the “noise,” or irrelevant information, within the dataset.\n4\n \nWhen the model memorizes the noise and fits too closely to the training set, the model \nbecomes “overfitted,” and it is unable to generalize well to new data. If a model cannot \ngeneralize well to new data, then it will not be able to perform the classification or prediction \ntasks it was intended for. However, if you pause too early or exclude too many important \nfeatures, you may encounter the opposite problem, and instead, you may “underfit” your \nmodel. Underfitting occurs when the model has not trained for enough time, or the input \nvariables are not significant enough to determine a meaningful relationship between the input \nand output variables. \n \nAI is also poor at estimating uncertainty /confidence (and explaining its decision-making). It \ncan’t choose its own goals. (Executives need to define the decision that the AI will execute.  \nWithout well-defined decisions to be made, data scientists will waste time, energy and money.) \nExcept for simple cases an AI can’t (yet) figure out cause and effect or why something \nhappened. It can’t think creatively or apply common sense.  \n \nAI is not very good at creating a strategy (unless it can pull from previous examples and mimic \nthem, but then fails with the unexpected.) And it lacks generalized intelligence e.g. that can \ngeneralize knowledge and translate learning across domains.  \n \nAll of these are research topics actively being worked on. Solving these will take a combination \nof high-performance computing, advanced AI/ML semiconductors, creative machine learning \nimplementations and decision science. Some may be solved in the next decade, at least to a \nlevel where a human can’t tell the difference. \n \nWhere is AI in Business Going Next? \nSkip this section if you’re interested in national security applications \n \nJust as classic computers were applied to a broad set of business, science and military \napplications, AI is doing the same. AI is exploding not only in research and infrastructure (which \ngo wide) but also in the application of AI to vertical problems (which go deep and depend more \nthan ever on expertise). Some of the new applications on the horizon include Human \nAI/Teaming (AI helping in programming and decision making), smarter robotics and \nautonomous vehicles, AI-driven drug discovery and design, healthcare diagnostics, chip \nelectronic design automation, and basic science research.  \n \n4\n https://www.ibm.com/cloud/learn/overfitting \n\n \n \nAdvances in language understanding are being pursued to create systems that can summarize \ncomplex inputs and engage through human-like conversation, a critical component of next-\ngeneration teaming. \n \nWhere is AI and National Security Going Next? \nIn the near future AI may be able to predict the future actions an adversary could take and the \nactions a friendly force could take to counter these. The 20\nth\n century model loop of Observe–\nOrient–Decide and Act (OODA) is retrospective; an observation cannot be made until after the \nevent has occurred. An AI-enabled decision-making cycle might be ‘sense–predict–agree–act’: \nAI senses the environment; predicts what the adversary might do and offers what a future \nfriendly force response should be; the human part of the human–machine team agrees with \nthis assessment; and AI acts by sending machine-to-machine instructions to the small, agile and \nmany autonomous warfighting assets deployed en masse across the battlefield. \n \nAn example of this is DARPA’s ACE (Air Combat Evolution) program that is developing a \nwarfighting concept for combined arms using a manned and unmanned systems. Humans will \nfight in close collaboration with autonomous weapon systems in complex environments with \ntactics informed by artificial intelligence. \n \nA Once-in-a-Generation Event \nImagine it’s the 1980’s and you’re in charge of an intelligence agency. SIGINT and COMINT were \nanalog and RF. You had worldwide collection systems with bespoke systems in space, air, \nunderwater, etc. And you wake up to a world that shifts from copper to fiber. Most of your \npeople, and equipment and equipment are going to be obsolete, and you need to learn how to \ncapture those new bits. Almost every business processes needed to change, new organizations \nneeded to be created, new skills were needed, and old ones were obsoleted. That’s what AI/ML \nis going to do to you and your agency. \n \nThe primary obstacle to innovation in national security is not technology, it is culture. The DoD \nand IC must overcome a host of institutional, bureaucratic, and policy challenges to adopting \nand integrating these new technologies. Many parts of our culture are resistant to change, \nreliant on traditional tradecraft and means of collection, and averse to risk-taking, (particularly \nacquiring and adopting new technologies and integrating outside information sources.)  \n \nHistory tells us that late adopters fall by the wayside as more agile and opportunistic \ngovernments master new technologies. \n \nCarpe Diem. \n \nWant more Detail?   \nRead on if you want to know about Machine Learning chips, see a sample Machine Learning \nPipeline and learning about the four types of Machine Learning. \n\n \n \nArtificial Intelligence/Machine Learning Semiconductors \nSkip this section if all you need to know is that special chips are used for AI/ML. \n \nAI/ML, semiconductors, and high-performance computing are intimately intertwined  - and \nprogress in each is dependent on the others.  (See the “Semiconductor Ecosystem” report.) \n \nSome machine learning models can have trillions of parameters and require a massive number \nof specialized AI chips to run. Edge computers are significantly less powerful than the massive \ncompute power that’s located at data centers and the cloud. They need low power and \nspecialized silicon. \n \nWhy Dedicated AI Chips and Chip Speed Matter \nDedicated chips for neutral nets (e.g. Nvidia GPUs, Xilinx FPUs, Google TPUs) are faster than \nconventional CPUs for three reasons: 1) they use parallelization, 2) they \nhave larger memory bandwidth and 3) they have fast memory access. \n \nThere are three types of AI Chips: \n• Graphics Processing Units (GPUs) - Thousands of cores, parallel \nworkloads, widespread use in machine learning \n• Field-Programmable Gate Arrays (FPGAs)  - Good for algorithms; \ncompression, video encoding, cryptocurrency,  genomics, search. \nNeeds specialists to program, \n• Application-Specific Integrated Circuits (ASICs) – custom chips e.g. \nGoogle TPU’s \n \nMatrix multiplication plays a big part in neural network computations, especially if there are \nmany layers and nodes. Graphics Processing Units (GPUs) contain 100s or 1,000s of cores that \ncan do these multiplications simultaneously. And neural networks are inherently parallel which \nmeans that it’s easy to run a program across the cores and clusters of these processors. That \nmakes AI chips 10s or even 1,000s of times faster and more efficient than classic CPUs for \ntraining and inference of AI algorithms. State-of-the-art AI chips are dramatically more cost-\neffective than state-of-the-art CPUs as a result of their greater efficiency for AI algorithms. \n \nCutting-edge AI systems require not only AI-specific chips, but state-of-the-art AI chips. Older AI \nchips incur huge energy consumption costs that quickly balloon to unaffordable levels. Using \nolder AI chips today means overall costs and slowdowns at least an order of magnitude greater \nthan for state-of- the-art AI chips.  \n \n\n \nCost and speed make it virtually impossible to develop and deploy cutting-edge AI algorithms \nwithout state-of-the-art AI chips. Even with state-of-the-art AI chips, training a large AI \nalgorithm can cost tens of millions of dollars and take weeks to \ncomplete. With general-purpose chips like CPUs or older AI chips, \nthis training would take much longer and cost orders of \nmagnitude more, making staying at the R&D frontier impossible. \nSimilarly, performing inference using less advanced or less \nspecialized chips could involve similar cost overruns and take \norders of magnitude longer.  \n \nIn addition to off-the-shelf AI chips from Nvidia, Xlinix and Intel, large companies like Facebook, \nGoogle, Amazon, have designed their own chips to accelerate AI. The opportunity is so large \nthat there are hundreds of AI accelerator startups designing their own chips, funded by 10’s of \nbillions of venture capital and private equity. None of these companies own a chip \nmanufacturing plant (a fab) so they all use a foundry (an independent company that makes \nchips for others) like TSMC in Taiwan (or SMIC in China for Defense related silicon.) \n \nA Sample of AI GPU, FPGA and ASIC AI Chips and Where They’re Made \n \n \n \nIP (  Intellectual Property) Vendors Also Offer AI Accelerators  \nAI chip designers can buy AI IP Cores – prebuilt AI accelerators from Synopsys (EV7x,) Cadence \n(Tensilica AI,) Arm (Ethos,) Ceva (SensPro2, NeuPro),  Imagination (Series4,) ThinkSilicon (Neox,) \nFlexLogic (eFPGA,) Edgecortix and others.  \n \nOther AI Hardware Architectures \nSpiking Neural Networks (SNN) is a completely different approach from Deep Neural Nets. A \nform of Neuromorphic computing it tries to emulate how a brain works. SNN neurons use \n\n \nsimple counters and adder—no matrix multiply hardware is needed and power consumption is \nmuch lower. SNNs are good at unsupervised learning – e.g. detecting patterns in unlabeled data \nstreams. Combined with their low power they’re a good fit for sensors at the edge. Examples:: \nBrainChip, GrAI Matter, Innatera, Intel. \n \nAnalog Machine Learning AI chips use analog circuits to do the matrix multiplication in memory. \nThe result is extremely low power AI for always-on sensors. Examples: Mythic (AMP,) Aspinity \n(AML100,) Tetramem. \n \nOptical (Photonics) AI Computation promise performance gains over standard digital silicon, \nand some are nearing production. They use intersecting coherent light beams rather than \nswitching transistors to perform matrix multiplies. Computation happens in picoseconds and \nrequires only power for the laser. (Though off-chip digital transitions still limit power savings.) \nExamples: Lightmatter, Lightelligence, Luminous, Lighton. \n \nAI Hardware for the Edge \nAs more AI moves to the edge, the Edge AI accelerator market is segmenting into high-end \nchips for camera-based systems and low-power chips for simple sensors. For example:  \n \nAI Chips in Autonomous vehicles, Augmented Reality and multicamera surveillance systems \nThese inference engines require high performance. Examples: Nvidia (Orin,) AMD (Versal,) \nQualcomm (Cloud AI 100,) and acquired Arriver for automotive software. \n \nAI Chips in Cameras for facial recognition, surveillance. These inference chips require a balance \nof processing power with low power. Putting an AI chip in each camera reduces latency and \nbandwidth. Examples: Hailo-8, Ambarella CV5S,  Quadric (Q16), (RealTek 3916N). \n \nUltralow-Power AI Chips Target IoT Sensors - IoT devices require very simple neural networks \nand can run for years on a single battery. Example applications: Presence detection, wakeword \ndetection, gunshot detection... Examples: Syntiant (NDP,) Innatera, BrainChip,,  \n \nAI/ML Hardware Benchmarks \nWhile there are lots of claims about how much faster each of these chips are for AI/ML there \nare now a set of standard benchmarks -  MLCommons. These benchmarks were created by \nGoogle, Baidu, Stanford, Harvard and U.C. Berkeley. \n \nOne Last Thing - Non-Nvidia AI Chips and the “Nvidia Software Moat” \nNew AI accelerator chips most deal with the software moat that Nvidia has built around their \nGPU’s. As popular AI applications and frameworks are built on Nvidia CUDA software platform,  \nif new AI Accelerator vendors want to port these applications to their chips they have to build \ntheir own drivers, compiler, debugger, and other tools. \n \n\n \nDetails of a machine learning pipeline \nThis is a sample of the workflow (a pipeline) data scientists use to develop, deploy and maintain \na machine learning model (see the detailed description here.) \n \n \n \nThe Types of Machine Learning\n5\n – skip this section if you want to believe it’s \nmagic. \n \nMachine Learning algorithms fall into four classes: \n \n1. Supervised Learning \n2. Unsupervised Learning \n3. Semi-supervised Learning \n4. Reinforcement Learning \n \nThey differ based on: \n• What types of data their algorithms can work with \n• For  supervised and unsupervised learning, whether or not the training data is labeled or \nunlabeled \n• How the system receives its data inputs  \n \n \n5\n https://www.ai.mil/docs/Understanding%20AI%20Technology.pdf \n\n \nSupervised Learning \n• A “supervisor” (a human or a software system) accurately labels each of the training data \ninputs with its correct associated output \n• Note that pre-labeled data is only required for the training data that \nthe algorithm uses to train the AI mode \n• In operation in the inference phase the AI will be generating its own \nlabels, the accuracy of which will depend on the AI’s training  \n• Supervised Learning can achieve extremely high performance, but they require very large, \nlabeled datasets \n• Using labeled inputs and outputs, the model can measure its accuracy and learn over time \n• For images a rule of thumb is that the algorithm needs at least 5,000 labeled examples of \neach category in order to produce an AI model with decent performance \n• In supervised learning, the algorithm “learns” from the training dataset by iteratively \nmaking predictions on the data and adjusting for the correct answer.  \n• While supervised learning models tend to be more accurate than unsupervised learning \nmodels, they require upfront human intervention to label the data appropriately. \n \nSupervised Machine Learning - Categories and Examples:  \n• Classification problems - use an algorithm to assign data into specific categories, such as \nseparating apples from oranges. Or classify spam in a separate folder from your inbox. \nLinear classifiers, support vector machines, decision trees and random forest are all \ncommon types of classification algorithms. \n• Regression - understands the relationship between dependent and independent variables. \nHelpful for predicting numerical values based on different data points, such as sales \nrevenue projections for a given business. Some popular regression algorithms are linear \nregression, logistic regression and polynomial regression. \n• Example algorithms include: Logistic Regression and the Back Propagation Neural Network \n \nUnsupervised Learning \n• These algorithms can analyze and cluster unlabeled data sets. They discover hidden \npatterns in data without the need for human intervention (hence, they are “unsupervised”) \n• They can extract features from the data without a label for the results  \n• For an image classifier, an unsupervised algorithm would not identify the image as a “cat” \nor a “dog.” Instead, it would sort the training dataset into various groups based on their \nsimilarity \n• Unsupervised Learning systems are often less predictable, but as unlabeled data is usually \nmore available than labeled data, they are important \n• Unsupervised algorithms are useful when developers want to understand their own \ndatasets and see what properties might be useful in either developing automation or \nchange operational practices and policies \n• They still require some human intervention for validating the output  \n \n            Understanding Artificial Intelligence Technology \n \n Gregory C. Allen | DoD Joint AI Center \n11 \n11 \nexpensive to develop, such as imagery classification, are often now affordable \nand sometimes even cheap.    \n \nDespite  their  huge  potential,  AI  solutions  are  not  a  great  fit  for  all  types  of \nproblems. If you have an application where you think using AI could be beneficial, \nknowing whether or not any particular system that is claiming to use “AI” is using \nMachine Learning is  important  for several  reasons.  For  one  thing, Machine \nLearning works differently from traditional software, and it has different strengths \nand  weaknesses  too.  Moreover, Machine  Learning tends  to  break and  fail in \ndifferent ways. A basic understanding of these strengths, weaknesses, and failure \nmodes can help you understand whether or not your particular problems are a \ngood fit for a Machine Learning AI solution.  \n \nWHAT ARE THE DIFFERENT TYPES OF MACHINE LEARNING? HOW DO THEY DIFFER? \nLike Artificial Intelligence, Machine Learning is also an umbrella term, and there \nare four different broad families of Machine Learning algorithms. There are also \nmany different subcategories and combinations under these four major families, \nbut  a  good  understanding  of  these four broad families will  be  sufficient  for  the \nvast majority of DoD employees, including senior leaders in non-technical roles.  \n \nThe four categories – \nexplained  more  on \nthe following page – \ndiffer based on what \ntypes  of  data  their \nalgorithms  can  work \nwith.   However, the \nimportant distinction \nis   not whether   the \ndata is audio, \nimages, text, or \nnumbers. Rather, the \nimportant  distinction \nis  whether  or  not  the  training  data  is  labeled or unlabeled  and  how  the  system \nreceives  its  data  inputs. Figure 3    provides  a  simple  illustration  of  labeled  and \nunlabeled training data for a classifier of images of cats and dogs. \n \nDepending upon whether or not data is labeled, a different family of algorithms \napplies. The four major   families   of   algorithms are   Supervised   Learning, \nUnsupervised Learning, Semi-Supervised Learning, and Reinforcement Learning. \n \nSupervised  Learning: “Supervised” means  that, before  the  algorithm  processes \nthe training data, some “supervisor” (which may be a human, group of humans, \nor  a  different  software  system)  has  accurately  labeled  each  of  the  data  inputs \n       Figure 3. Labeled and Unlabeled Training Data \n\n \nUnsupervised Machine Learning - Categories and Examples \n• Clustering groups unlabeled data based on their similarities or differences. For example, K-\nmeans clustering algorithms assign similar data points into groups, where the K value \nrepresents the size of the grouping and granularity. This technique is helpful for market \nsegmentation, image compression, etc. \n• Association finds relationships between variables in a given dataset. These \nmethods are frequently used for market basket analysis and \nrecommendation engines, along the lines of “Customers Who Bought This \nItem Also Bought” recommendations. \n• Dimensionality reduction is used when the number of features  (or \ndimensions) in a given dataset is too high. It reduces the number of data \ninputs to a manageable size while also preserving the data integrity. Often, \nthis technique is used in the preprocessing data stage, such as when \nautoencoders remove noise from visual data to improve picture quality. \n• Example algorithms include: Apriori algorithm and K-Means \n \nDifference between supervised and unsupervised learning  \nThe main difference: Labeled data \n• Goals: In supervised learning, the goal is to predict outcomes for new data. You know up \nfront the type of results to expect. With an unsupervised learning algorithm, the goal is to \nget insights from large volumes of new data. The machine learning itself determines what is \ndifferent or interesting from the dataset. \n• Applications: Supervised learning models are ideal for spam detection, sentiment analysis, \nweather forecasting and pricing predictions, among other things. In contrast, unsupervised \nlearning is a great fit for anomaly detection, recommendation engines, customer personas \nand medical imaging. \n• Complexity: Supervised learning is a simple method for machine learning, typically \ncalculated through the use of programs like R or Python. In unsupervised learning, you need \npowerful tools for working with large amounts of unclassified data. Unsupervised learning \nmodels are computationally complex because they need a large training set to produce \nintended outcomes. \n• Drawbacks: Supervised learning models can be time-consuming to train, and the labels for \ninput and output variables require expertise. Meanwhile, unsupervised learning methods \ncan have wildly inaccurate results unless you have human intervention to validate the \noutput variables. \n \nSemi-Supervised Learning \n• “Semi- Supervised” algorithms combine techniques from Supervised and Unsupervised \nalgorithms for applications with a small set of labeled data and a large set of unlabeled data.  \n• In practice, using them leads to exactly what you would expect, a mix of some of both of the \nstrengths and weaknesses of Supervised and Unsupervised approaches \n            Understanding Artificial Intelligence Technology \n \n Gregory C. Allen | DoD Joint AI Center \n13 \n13 \n \nFigure 4 depicts the differences between Supervised and Unsupervised algorithms \nusing an image analysis example. \n \nFigure 4. Illustrated Example of Supervised and Unsupervised Algorithms \n \nIt  is  not  true  that  Unsupervised  Learning  is  “worse”  than  Supervised Learning \n(though  performance  can  be  lower  for some use  cases).  Rather, Unsupervised \nLearning is useful for solving different types of problems. A common Unsupervised \nuse  case  is  fraud  detection  in  financial  data. In  the  case  of  fraud  detection, \nSupervised  Learning  could  be  a  good  fit  for  identifying  potential  fraud  that \nmatches behaviors known to be unlawful or associated with fraud. Unsupervised \nLearning can find new, unidentified patterns of behavior that might indicate new \ntypes of fraud techniques. \n \n\n \n• Typical algorithms are extensions to other flexible methods that make assumptions about \nhow to model the unlabeled data. An example is \nGenerative Adversarial Networks trained on \nphotographs can generate new photographs that look \nauthentic to human observers (deep fakes)  \n \nReinforcement Learning \n• Training data is collected by an autonomous, self-directed AI agent as it perceives its \nenvironment and performs goal-directed actions  \n• The rewards are input data received by the AI agent \nwhen certain criteria are satisfied.  \n• These criteria are typically unknown to the agent at \nthe start of training \n• Rewards often contain only partial information. \nThey don’t signal which inputs were good or not \n• The system is learning to take actions to maximize its receipt of cumulative rewards \n• Reinforcement AI can defeat humans– in chess, Go...  \n• There are no labeled datasets for every possible move  \n• There is no assessment of whether it was a “good or bad move \n• Instead, partial labels reveal the final outcome “win” or “lose”  \n• The algorithms explore the space of possible actions to learn the optimal set of rules for \ndetermining the best action that maximize wins  \n \nReinforcement Machine Learning - Categories and Examples \n• Algorithm examples include: DQN (Deep Q Network), DDPG (Deep Deterministic Policy \nGradient), A3C (Asynchronous Advantage Actor-Critic Algorithm), NAF (Q-Learning with \nNormalized Advantage Functions), ... \n• AlphaGo, a Reinforcement system played 4.9 million \ngames of Go in 3 days against itself to learn how to \nplay the game at a world-champion level \n• Reinforcement is challenging to use in the real \nworld, as the real world is not as heavily bounded as \nvideo games and time cannot be sped up in the real \nworld \n• There are consequences to failure in the real world \n \nSources: \n• Understanding AI Technology: Greg Allen, Chief of Strategy and Communications Joint \nArtificial Intelligence Center (JAIC), Department of Defense \nhttps://www.ai.mil/docs/Understanding%20AI%20Technology.pdf \n            Understanding Artificial Intelligence Technology \n \n Gregory C. Allen | DoD Joint AI Center \n14 \n14 \nSemi-Supervised  Learning: There  is  also  an  increasingly  popular  class  of  “Semi-\nSupervised”    algorithms    that    combine    techniques    from Supervised    and \nUnsupervised algorithms for applications with a small set of labeled data and a \nlarge  set  of  unlabeled  data.  In  practice,  using  them  leads  to  exactly  what  you \nwould  expect,  a  mix  of some  of both  of  the  strengths  and  weaknesses  of \nSupervised and Unsupervised approaches. \n \nReinforcement Learning:  In Reinforcement Learning, the training data is collected \nby  an autonomous,  self-directed AI  agent  in  the  course  of perceiving  its \nenvironment (which  might  be  the  real  world  or  a  simulated  environment)  and \nperforming goal-directed actions (trying to maximize receipt of “rewards”). Four \naspects  of Reinforcement  Learning are  notably  distinct  from Supervised  and \nUnsupervised Learning: \n \n1) Data is gathered by the AI agent itself in the course of its interacting with \nthe environment and perceiving stated changes. For example, an AI agent \nplaying  a  digital  game  of chess  makes  moves  and  perceives  changes  in \nthe board based on its moves. \n2) The rewards are input data received by the agent when certain criteria are \nsatisfied. For  example,  a Reinforcement  Learning AI  agent  in  chess  will \nmake  many  moves  before  each  win  or  loss. These  criteria  are  typically \nunknown to the agent at the outset of training. \n3) Rewards often contain only partial information. A reward like a win in chess \nconveys  that  some  inputs  must  have  been  good,  but  it  doesn’t  clearly \nsignal which inputs were good and which were not.  \n4) The  system  is  learning an  action  policy for taking  actions  to maximize  its \nreceipt of cumulative rewards.   \n \nFigure 5: Simplified Reinforcement Learning Diagram \n \n\n \n• AI, Machine Learning, Deep Learning Explained Simply: Jun Wu \nhttps://towardsdatascience.com/ai-machine-learning-deep-learning-explained-simply-\n7b553da5b960 \n• The Democratization of Artificial Intelligence and Deep Learning: Databricks \nhttps://databricks.com/discover/pages/the-democratization-of-artificial-intelligence-and-\ndeep-learning \n• Final Report: National Security Report on Artificial Intelligence https://www.nscai.gov/wp-\ncontent/uploads/2021/03/Full-Report-Digital-1.pdf \n• A Beginners Guide to Neural Nets and Deep Learning: Pathmind \nhttps://wiki.pathmind.com/neural-network \n \n \n \n \n \n \n \n \n ",
      "questions": [
        "If you haven’t paid attention, now’s the time?",
        "Imagine it’s 1950, and you’re a visitor who traveled back in time from today. Your job is to explain the impact computers will have on business, defense, and society to people who are using manual calculators and slide rules?",
        "But What Is It?",
        "Why is Machine Learning Possible Now?"
      ],
      "answers": {
        "Why is Machine Learning Possible Now?": "Machine learning is possible now because it allows computers to \"learn by example\" through training with large volumes of data, which was not possible with traditional programming on classic computers that operated based on fixed rules. This training process enables computers to make predictions and complex decisions autonomously once the model is trained."
      }
    },
    {
      "id": "d6109cd0-e1b1-4511-ae76-2f6432556f6c",
      "filename": "Invoice.pdf",
      "text": "\n\nInvoice\n7 February 2024\nBILL TO:\nNeema\nMwende Mbiti\n+254792366778\nMONTHPRICE\nTAX (0%)SUBTOTAL\nSUBTOTAL\nTOTALS\nJanuary 2025$250.00\n0.00\n$250.00\n$250.00\n1\nPayment Information\nRuaka  00618, Nairobi \n+254 792 366 778\nneemamwende009@gmail.com\nNeema Mwende Mbiti\nBank: Equity Bank\nAccount No: 0670185946128",
      "questions": [],
      "answers": {}
    }
  ]
}